{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set 3\n",
    "\n",
    "Author: Andre Oviedo Mendoza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1) {-}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1 {-}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will need to reorder the observations so that our first $n_0$ rows are $s_o$ and the last $n_1$ rows are $s_1$.\n",
    "\n",
    "$\\boldsymbol{S}^{\\prime} \\boldsymbol{S}$ will be a diagonal matrix where each element of the diagonal entry are the number of observations with $S_i = s_i$.\n",
    "\n",
    "Therefore, \n",
    "\n",
    "$$\\operatorname{plim}(\\frac{1}{n}\\boldsymbol{S}^{\\prime} \\boldsymbol{S}) = \\text{diag}(\\mathbb{P}(S=s_0), \\mathbb{P}(S=s_1), ..., \\mathbb{P}(S=s_L))$$\n",
    "\n",
    "For $\\boldsymbol{S}^{\\prime}T$, each entry is the sum of T values for observations with $S_i = s_l$, so:\n",
    "\n",
    "$$\\operatorname{plim}(\\frac{1}{n}\\boldsymbol{S}^{\\prime}T) = (\\mathbb{P}(T=1|S=s_0)\\mathbb{P}(S=s_0), ..., \\mathbb{P}(T=1|S=s_L)\\mathbb{P}(S=s_L))'$$\n",
    "\n",
    "Our first-stage coefficients are\n",
    "\n",
    "$$\\operatorname{plim}(\\frac{1}{n}\\boldsymbol{S}^{\\prime} \\boldsymbol{S})^{-1}\\operatorname{plim}(\\frac{1}{n}\\boldsymbol{S}^{\\prime}T) = (g(s_0), g(s_1), ..., g(s_L))'$$\n",
    "\n",
    "We know that\n",
    "\n",
    "$$\\operatorname{plim}(\\frac{1}{n}\\boldsymbol{S}^{\\prime} \\boldsymbol{S})$$ \n",
    "\n",
    "is a diagonal matrix with $\\mathbb{P}(S=s_l)$ on the diagonal, and\n",
    "\n",
    "$$\\operatorname{plim}(\\frac{1}{n}\\boldsymbol{S}^{\\prime}T)$$ \n",
    "\n",
    "is a vector with entries $\\mathbb{P}(T=1|S=s_l)\\mathbb{P}(S=s_l)$\n",
    "\n",
    "When we multiply the inverse of the first matrix by the second vector. The $\\mathbb{P}(S=s_l)$ terms cancel out and we're left with just $\\mathbb{P}(T=1|S=s_l)$ for each level $l$.\n",
    "\n",
    "From the threshold crossing model, we know $\\mathbb{P}(T=1|S=s_l) = g(s_l)$.\n",
    "\n",
    "Therefore, the first-stage coefficient on each indicator $$\\mathbb{1}(S_i = s_l)$$ equals $$g(s_l)$$, which is what we wanted to prove."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2 {-}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, write out the covariance:\n",
    "$$ \\operatorname{Cov}(Y,g(S)) = \\mathbb{E}[Y g(S)] - \\mathbb{E}[Y]\\mathbb{E}[g(S)] $$\n",
    "\n",
    "by using the law of total expectation, we know that $ \\mathbb{E}[Y g(S)] = \\sum_{l=0}^{L} \\mathbb{E}[Y|S=s_l]g(s_l)\\mathbb{P}(S=s_l) $\n",
    "\n",
    "\n",
    "For each conditional expectation, using the fact that $g(s)$ is increasing in $s$ we can use the telescoping sum to rewrite $ \\mathbb{E}[Y|S=s_l] $ as\n",
    "\n",
    "$$ \\mathbb{E}[Y|S=s_l] = \\mathbb{E}[Y|S=s_0] + \\sum_{m=1}^{l}(\\mathbb{E}[Y|S=s_m] - \\mathbb{E}[Y|S=s_{m-1}]) $$\n",
    "\n",
    "Substituting this into our covariance formula,\n",
    "\n",
    "$$ \\operatorname{Cov}(Y,g(S)) = \\sum_{l=0}^{L} \\left(\\mathbb{E}[Y|S=s_0] + \\sum_{m=1}^{l}(\\mathbb{E}[Y|S=s_m] - \\mathbb{E}[Y|S=s_{m-1}])\\right)g(s_l)\\mathbb{P}(S=s_l) - \\mathbb{E}[Y]\\mathbb{E}[g(S)] $$\n",
    "\n",
    "and after distributing terms and rearranging we arrive to our desired result:\n",
    "\n",
    "$$ \\operatorname{Cov}(Y,g(S)) = \\sum_{l=0}^{L}\\left(\\sum_{m=1}^{l}\\left[\\mathbb{E}\\left[Y \\mid S=s_{m}\\right]-\\mathbb{E}\\left[Y \\mid S=s_{m-1}\\right]\\right]\\right)\\left(g\\left(s_{l}\\right)-\\mathbb{E}[g(S)]\\right) \\mathbb{P}\\left(S=s_{l}\\right) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3 {-}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start fro the result from Problem 2\n",
    "\n",
    "$$ \\operatorname{Cov}(Y,g(S)) = \\sum_{l=0}^{L}\\left(\\sum_{m=1}^{l}\\left[\\mathbb{E}\\left[Y \\mid S=s_{m}\\right]-\\mathbb{E}\\left[Y \\mid S=s_{m-1}\\right]\\right]\\right)\\left(g\\left(s_{l}\\right)-\\mathbb{E}[g(S)]\\right) \\mathbb{P}\\left(S=s_{l}\\right) $$\n",
    "\n",
    "Following Chamberlain's notes, we know that for any $s$ \n",
    "\n",
    "$$ \\mathbb{E}[Y|S=s] = \\mathbb{E}[Y(0)] + g(s)\\theta + \\int_0^{g(s)} \\mathbb{E}[U_1-U_0|V=v]dv $$\n",
    "\n",
    "Therefore\n",
    "\n",
    "$$ \\mathbb{E}[Y|S=s_m] - \\mathbb{E}[Y|S=s_{m-1}] = [g(s_m) - g(s_{m-1})]\\mathbb{E}[Y(1)-Y(0)|g(s_{m-1}) \\leq V \\leq g(s_m)] $$\n",
    "\n",
    "From the defintion of the $\\operatorname{LATE}_{m-1}^m$ we know that\n",
    "\n",
    "$$ \\operatorname{LATE}_{m-1}^m = \\mathbb{E}[Y(1)-Y(0)|g(s_{m-1}) \\leq V \\leq g(s_m)] $$\n",
    "\n",
    "So, rearranging the terms in our covariance formula and replacing the $\\operatorname{LATE}_{m-1}^m$ we get\n",
    "\n",
    "$$ \\operatorname{Cov}(Y,g(S)) = \\sum_{l=0}^{L}\\left(\\sum_{m=1}^{l}[g(s_m) - g(s_{m-1})]\\operatorname{LATE}_{m-1}^m\\right)\\left(g\\left(s_{l}\\right)-\\mathbb{E}[g(S)]\\right) \\mathbb{P}\\left(S=s_{l}\\right) $$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4 {-}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To show that both expressions are equivalent, we have to understand the LHS\n",
    "\n",
    "In this expression\n",
    "\n",
    "- We first sum over $l$ from 0 to $L$.\n",
    "- For each $l$, we sum over $m$ from 1 to $l$.\n",
    "- The term inside the summation over $m$ represents the difference in values of $g(s_m)$ and $g(s_{m-1})$, multiplied by the local average treatment effect (LATE) between these two points.\n",
    "- Finally, we multiply by the difference $g(s_l) - \\mathbb{E}[g(S)]$, weighted by the probability $\\mathbb{P}(S = s_l)$.\n",
    "\n",
    "The key step in this problem is to **switch the order of summation**. Instead of summing over $l$ first and then $m$, we will sum over $m$ first and then $l$. This is valid because we are just reordering how we sum the terms, which does not change their values.\n",
    "\n",
    "This works because:\n",
    "- In the original expression, for each fixed $l$, we sum over all $m$ such that $m \\leq l$.\n",
    "- After switching the order, for each fixed $m$, we sum over all $l$ such that $l \\geq m$.\n",
    "\n",
    "This reordering works because every term in the original double summation appears exactly once, and changing the order simply reorganizes how we group those terms.\n",
    "\n",
    "$$\n",
    "\\sum_{m=1}^{L} \\mathrm{LATE}_{m-1}^{m} \\cdot \\left( g(s_m) - g(s_{m-1}) \\right) \\cdot \\sum_{l=m}^{L} \\left( g(s_l) - \\mathbb{E}[g(S)] \\right) P(S = s_l)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5 {-}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From Problem 2, we can use the same approach to write Cov(T,g(S)) in terms of differences in conditional expectations:\n",
    "\n",
    "$$ \\operatorname{Cov}(T, g(S))=\\sum_{l=0}^{L}\\left(\\sum_{m=1}^{l}\\left[\\mathbb{E}\\left[T \\mid S=s_{m}\\right]-\\mathbb{E}\\left[T \\mid S=s_{m-1}\\right]\\right]\\right)\\left(g\\left(s_{l}\\right)-\\mathbb{E}[g(S)]\\right) \\mathbb{P}\\left(S=s_{l}\\right) $$\n",
    "\n",
    "From Problem 1, we know that\n",
    "\n",
    "$$ \\mathbb{E}[T|S=s_l] = g(s_l) $$\n",
    "\n",
    "Therefore\n",
    "\n",
    "$$ \\mathbb{E}[T|S=s_m] - \\mathbb{E}[T|S=s_{m-1}] = g(s_m) - g(s_{m-1}) $$\n",
    "\n",
    "Using Problem 4's approach to change the order of summation, we get\n",
    "\n",
    "$$ \\operatorname{Cov}(T, g(S))=\\sum_{m=1}^{L}\\left(g\\left(s_{m}\\right)-g\\left(s_{m-1}\\right)\\right) \\sum_{l=m}^{L}\\left(g\\left(s_{l}\\right)-\\mathbb{E}[g(S)]\\right) \\mathbb{P}\\left(S=s_{l}\\right) $$\n",
    "\n",
    "This proves the desired result. The covariance between $T$ and $g(S)$ can be written as a weighted sum of the differences in $g(S)$ at adjacent levels, where the weights involve the probabilities and deviations from the mean at higher levels of the instrument."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 6 {-}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have already calculated the components of $\\beta_\\text{2SLS}$ in Problem 5 and 4.\n",
    "\n",
    "$$ \\beta_\\text{2SLS} = \\frac{\\operatorname{Cov}(Y,g(S))}{\\operatorname{Cov}(T,g(S))} $$\n",
    "\n",
    "Substituting the components we found in Problems 5 and 4, we get\n",
    "\n",
    "$$ \\beta_\\text{2SLS} = \\frac{\\sum_{m=1}^{L} \\mathrm{LATE}_{m-1}^{m} \\cdot \\left( g(s_m) - g(s_{m-1}) \\right) \\cdot \\sum_{l=m}^{L} \\left( g(s_l) - \\mathbb{E}[g(S)] \\right) P(S = s_l)}{\\sum_{m=1}^{L}\\left(g\\left(s_{m}\\right)-g\\left(s_{m-1}\\right)\\right) \\sum_{l=m}^{L}\\left(g\\left(s_{l}\\right)-\\mathbb{E}[g(S)]\\right) \\mathbb{P}\\left(S=s_{l}\\right)} $$\n",
    "\n",
    "From this equation we can extract a set of weights $w_m$ that are the coefficients on $\\mathrm{LATE}_{m-1}^m$ in the 2SLS regression:\n",
    "\n",
    "$$ w_m = \\frac{(g(s_m) - g(s_{m-1})) \\sum_{l=m}^{L}(g(s_l) - \\mathbb{E}[g(S)])\\mathbb{P}(S=s_l)}{\\sum_{m=1}^{L}(g(s_m) - g(s_{m-1})) \\sum_{l=m}^{L}(g(s_l) - \\mathbb{E}[g(S)])\\mathbb{P}(S=s_l)} $$\n",
    "\n",
    "More importantly, we can see that the weights sum to one by construction: the numerator is an element of the (summed) denominator. Will the weights always be positve? We can extract the expected sign of each of the components of $w_m$ from the expression:\n",
    "\n",
    "- $g(s_m) - g(s_{m-1}) > 0$ by assumption   \n",
    "- The sum $\\sum_{l=m}^{L}(g(s_l) - \\mathbb{E}[g(S)])P(S=s_l)$ is positive for $l \\geq m$ since $g(s)$ is increasing\n",
    "- The denominator is positive as it's a sum of positive terms\n",
    "\n",
    "This weighted average interpretation is desirable because:\n",
    "- It shows $\\beta_\\text{2SLS}$ is a proper convex combination of local average treatment effects\n",
    "- The positive weights ensure the estimate lies between the minimum and maximum LATEs\n",
    "- Each LATE is weighted by both the size of the complier group and the instrument's strength at that level\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2) {-}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 7 {-}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The instrumental variables (IV) estimator for $\\beta$ solves the following moment condition:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[Z(Y - D\\beta)] = 0\n",
    "$$\n",
    "\n",
    "This implies that the IV estimator $\\hat{\\beta}$ is obtained by solving:\n",
    "\n",
    "$$\n",
    "\\hat{\\beta} = \\left( Z'D \\right)^{-1} Z'Y\n",
    "$$\n",
    "\n",
    "where $Z$ is the matrix of instruments, $D$ is the endogenous regressor, and $Y$ is the outcome variable.\n",
    "\n",
    "To construct confidence intervals for $\\hat{\\beta}$, we can use **inverted hypothesis tests** based on the moment condition. The idea is to test whether a particular value of $\\beta_0$ satisfies the moment condition:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[Z(Y - D\\beta_0)] = 0\n",
    "$$\n",
    "\n",
    "For a given value of $\\beta_0$, we can compute the sample analog of this moment condition:\n",
    "\n",
    "$$\n",
    "m(\\beta_0) = \\frac{1}{n} Z'(Y - D\\beta_0)\n",
    "$$\n",
    "\n",
    "If $m(\\beta_0)$  is close to zero, then $\\beta_0$ is a plausible value for $\\beta$. If not, we reject $\\beta_0$.\n",
    "\n",
    "We can construct a **test statistic** based on $m(\\beta_0)$. A natural choice is a quadratic form that accounts for the covariance structure of the moment conditions. Specifically, we can use a Wald-type test statistic\n",
    "\n",
    "$$\n",
    "T(\\beta_0) = m(\\beta_0)' W^{-1} m(\\beta_0)\n",
    "$$\n",
    "\n",
    "where $W^{-1}$ is an estimate of the covariance matrix of the moment conditions. Under the null hypothesis that $\\mathbb{E}[Z(Y - D\\beta_0)] = 0$, this test statistic follows a chi-squared distribution with degrees of freedom equal to the number of instruments.\n",
    "\n",
    "To construct confidence intervals for $\\hat{\\beta}$, we can invert this hypothesis test. Specifically, we find all values of $\\beta_0$ such that the test statistic $T(\\beta_0)$ does not exceed a critical value from the chi-squared distribution.\n",
    "\n",
    "Formally, let $c_{\\alpha}$ be the critical value from the chi-squared distribution at significance level $\\alpha$. The **confidence interval** for $\\hat{\\beta}$ is given by:\n",
    "\n",
    "$$\n",
    "CI_{1-\\alpha} = \\left\\{ \\beta_0 : T(\\beta_0) \\leq c_{\\alpha} \\right\\}\n",
    "$$\n",
    "\n",
    "This approach ensures that we include all values of $\\beta_0$ that are consistent with the moment condition within a certain confidence level.\n",
    "\n",
    "In practice, you would:\n",
    "1. Estimate $m(\\beta_0)$ for different candidate values of $\\beta_0$.\n",
    "2. Compute the test statistic $T(\\beta_0)$.\n",
    "3. Compare $T(\\beta_0)$ to the critical value from a chi-squared distribution.\n",
    "4. Construct confidence intervals by finding all values of $\\beta_0$ where $T(\\beta_0)$ does not exceed this critical value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3) {-}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 8 {-}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source the Python script\n",
    "exec(open('dml_example.py').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 8 {-}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:               logghomr   R-squared:                       0.111\n",
      "Model:                            OLS   Adj. R-squared:                  0.111\n",
      "Method:                 Least Squares   F-statistic:                     487.7\n",
      "Date:                Tue, 19 Nov 2024   Prob (F-statistic):          6.26e-102\n",
      "Time:                        00:16:25   Log-Likelihood:                -5661.6\n",
      "No. Observations:                3900   AIC:                         1.133e+04\n",
      "Df Residuals:                    3898   BIC:                         1.134e+04\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          0.3288      0.044      7.480      0.000       0.243       0.415\n",
      "logfssl        0.3020      0.014     22.084      0.000       0.275       0.329\n",
      "==============================================================================\n",
      "Omnibus:                        6.262   Durbin-Watson:                   0.296\n",
      "Prob(Omnibus):                  0.044   Jarque-Bera (JB):                6.315\n",
      "Skew:                          -0.094   Prob(JB):                       0.0425\n",
      "Kurtosis:                       2.941   Cond. No.                         9.26\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "# Using the variables from the data:\n",
    "\n",
    "# y = \"logghomr\" (log gun homicide rate)\n",
    "# d = \"logfssl\" (log gun ownership proxy)\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "simple_model = sm.OLS(usedata['logghomr'], sm.add_constant(usedata['logfssl'])).fit()\n",
    "print(simple_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline OLS: 0.30202337690461406 ( 0.012565715795207547 )\n"
     ]
    }
   ],
   "source": [
    "# This gives similar coef estimates as the code from the book (which fixes the std err)\n",
    "\n",
    "# Baseline OLS\n",
    "X = sm.add_constant(usedata['logfssl'])\n",
    "y = usedata['logghomr']\n",
    "lm0 = sm.OLS(y, X).fit(cov_type='HC3')\n",
    "vc0 = lm0.cov_params()\n",
    "coef = lm0.params['logfssl']\n",
    "std_err = np.sqrt(vc0.loc['logfssl', 'logfssl'])\n",
    "print(\"Baseline OLS:\", coef, \"(\", std_err, \")\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this to be interpreted as a causal effect, we would need several key identification assumptions:\n",
    "\n",
    "\n",
    "1. **Exogeneity/Unconfoundedness**: Gun ownership levels must be *as good as* randomly assigned, meaning there are no unmeasured factors that affect both gun ownership and homicide rates. This is a strong assumption because:\n",
    "   - Areas with higher crime might have more people buying guns for protection\n",
    "   - Cultural and socioeconomic factors likely influence both gun ownership and violence\n",
    "   - Local policies could affect both variables  \n",
    "2. **Stable Unit Treatment Value Assumption (SUTVA)**:\n",
    "   - One area's gun ownership levels shouldn't affect homicide rates in other areas\n",
    "   - This might be violated due to cross-border gun trafficking or spillover effects\n",
    "3. **No Reverse Causality**:\n",
    "   - Changes in homicide rates shouldn't cause changes in gun ownership\n",
    "   - This is likely violated as people may buy guns in response to crime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 9 {-}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create formula with all control variables\n",
    "controls = [d] + X1 + X2 + census  # Combining all control variables\n",
    "X = usedata[controls]\n",
    "X = sm.add_constant(X)  # Add constant term\n",
    "X['logfssl'] = usedata['logfssl']  # Add treatment variable\n",
    "\n",
    "controlled_model = sm.OLS(usedata['logghomr'], X).fit()\n",
    "#print(controlled_model.summary()) Not printing cause of length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OLS with Controls: 0.3102764669361729 ( 0.0729464029073983 )\n"
     ]
    }
   ],
   "source": [
    "# Regression on baseline controls from the book, again, similar answer\n",
    "varlist = [d] + X1 + X2 + census\n",
    "X = sm.add_constant(usedata[varlist])\n",
    "y = usedata['logghomr']\n",
    "lmC = sm.OLS(y, X).fit(cov_type='HC3')\n",
    "vcC = lmC.cov_params()\n",
    "coef = lmC.params['logfssl']\n",
    "std_err = np.sqrt(vcC.loc['logfssl', 'logfssl'])\n",
    "print(\"OLS with Controls:\", coef, \"(\", std_err, \")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also follow the book and add all controls\n",
    "X = sm.add_constant(usedata.drop(columns=['logghomr'])) \n",
    "controlled_model = sm.OLS(usedata['logghomr'], X).fit()\n",
    "#print(controlled_model.summary()) Not printing cause of length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OLS All: 0.17931590182937693 ( 0.07981180505737272 )\n"
     ]
    }
   ],
   "source": [
    "# From the book\n",
    "\n",
    "X = sm.add_constant(usedata.drop(columns=['logghomr']))\n",
    "y = usedata['logghomr']\n",
    "lmA = sm.OLS(y, X).fit(cov_type='HC3')\n",
    "vcA = lmA.cov_params()\n",
    "coef = lmA.params['logfssl']\n",
    "std_err = np.sqrt(vcA.loc['logfssl', 'logfssl'])\n",
    "print(\"OLS All:\", coef, \"(\", std_err, \")\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, for this controlled effect to be interpreted as causal, we still need a lot of assumptions for this to be considered a causal effect. \n",
    "\n",
    "The main difficulty that this could have is the exogeneity condition. Gun ownership levels must still be as a good as randomly assigned, even after controlling for all other variables. This could be a really hard condition to fulfill:\n",
    "\n",
    "- Areas with higher crime might have more people buying guns for protection\n",
    "- Cultural and socioeconomic factors likely influence both gun ownership and violence\n",
    "- Local policies could affect both variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 10 {-}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dml(X, D, y, modely, modeld, *, nfolds, classifier=False):\n",
    "    '''\n",
    "    DML for the Partially Linear Model setting with cross-fitting\n",
    "\n",
    "    Input\n",
    "    -----\n",
    "    X: the controls\n",
    "    D: the treatment\n",
    "    y: the outcome\n",
    "    modely: the ML model for predicting the outcome y\n",
    "    modeld: the ML model for predicting the treatment D\n",
    "    nfolds: the number of folds in cross-fitting\n",
    "    classifier: bool, whether the modeld is a classifier or a regressor\n",
    "\n",
    "    time: array of time indices, eg [0,1,...,T-1,0,1,...,T-1,...,0,1,...,T-1]\n",
    "    clu: array of cluster indices, eg [1073, 1073, 1073, ..., 5055, 5055, 5055, 5055]\n",
    "    cluster: bool, whether to use clustered standard errors\n",
    "\n",
    "    Output\n",
    "    ------\n",
    "    point: the point estimate of the treatment effect of D on y\n",
    "    stderr: the standard error of the treatment effect\n",
    "    yhat: the cross-fitted predictions for the outcome y\n",
    "    Dhat: the cross-fitted predictions for the treatment D\n",
    "    resy: the outcome residuals\n",
    "    resD: the treatment residuals\n",
    "    epsilon: the final residual-on-residual OLS regression residual\n",
    "    '''\n",
    "    cv = KFold(n_splits=nfolds, shuffle=True, random_state=123)  # shuffled k-folds\n",
    "    yhat = cross_val_predict(modely, X, y, cv=cv, n_jobs=-1)  # out-of-fold predictions for y\n",
    "    # out-of-fold predictions for D\n",
    "    # use predict or predict_proba dependent on classifier or regressor for D\n",
    "    if classifier:\n",
    "        Dhat = cross_val_predict(modeld, X, D, cv=cv, method='predict_proba', n_jobs=-1)[:, 1]\n",
    "    else:\n",
    "        Dhat = cross_val_predict(modeld, X, D, cv=cv, n_jobs=-1)\n",
    "    # calculate outcome and treatment residuals\n",
    "    resy = y - yhat\n",
    "    resD = D - Dhat\n",
    "\n",
    "    dml_data = pd.concat([pd.Series(resy, name='resy'), pd.Series(resD, name='resD')], axis=1)\n",
    "    ols_mod = smf.ols(formula='resy ~ 1 + resD', data=dml_data).fit()\n",
    "\n",
    "    point = ols_mod.params[1]\n",
    "    stderr = ols_mod.bse[1]\n",
    "    epsilon = ols_mod.resid\n",
    "\n",
    "    return point, stderr, yhat, Dhat, resy, resD, epsilon\n",
    "\n",
    "def summary(point, stderr, yhat, Dhat, resy, resD, epsilon, X, D, y, *, name):\n",
    "    '''\n",
    "    Convenience summary function that takes the results of the DML function\n",
    "    and summarizes several estimation quantities and performance metrics.\n",
    "    '''\n",
    "    return pd.DataFrame({'estimate': point,  # point estimate\n",
    "                         'stderr': stderr,  # standard error\n",
    "                         'lower': point - 1.96 * stderr,  # lower end of 95% confidence interval\n",
    "                         'upper': point + 1.96 * stderr,  # upper end of 95% confidence interval\n",
    "                         'rmse y': np.sqrt(np.mean(resy**2)),  # RMSE of model that predicts outcome y\n",
    "                         'rmse D': np.sqrt(np.mean(resD**2))  # RMSE of model that predicts treatment D\n",
    "                         }, index=[name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>estimate</th>\n",
       "      <th>stderr</th>\n",
       "      <th>lower</th>\n",
       "      <th>upper</th>\n",
       "      <th>rmse y</th>\n",
       "      <th>rmse D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NN 1 layers 32 units</th>\n",
       "      <td>0.189524</td>\n",
       "      <td>0.046316</td>\n",
       "      <td>0.098744</td>\n",
       "      <td>0.280304</td>\n",
       "      <td>0.505605</td>\n",
       "      <td>0.174429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NN 2 layers 32 units</th>\n",
       "      <td>0.092973</td>\n",
       "      <td>0.042845</td>\n",
       "      <td>0.008997</td>\n",
       "      <td>0.176950</td>\n",
       "      <td>0.500504</td>\n",
       "      <td>0.186808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NN 2 layers 64 units</th>\n",
       "      <td>0.229418</td>\n",
       "      <td>0.039138</td>\n",
       "      <td>0.152707</td>\n",
       "      <td>0.306128</td>\n",
       "      <td>0.504359</td>\n",
       "      <td>0.208103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NN 3 layers 32 units</th>\n",
       "      <td>-0.079973</td>\n",
       "      <td>0.032738</td>\n",
       "      <td>-0.144140</td>\n",
       "      <td>-0.015806</td>\n",
       "      <td>0.480433</td>\n",
       "      <td>0.247671</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      estimate    stderr     lower     upper    rmse y  \\\n",
       "NN 1 layers 32 units  0.189524  0.046316  0.098744  0.280304  0.505605   \n",
       "NN 2 layers 32 units  0.092973  0.042845  0.008997  0.176950  0.500504   \n",
       "NN 2 layers 64 units  0.229418  0.039138  0.152707  0.306128  0.504359   \n",
       "NN 3 layers 32 units -0.079973  0.032738 -0.144140 -0.015806  0.480433   \n",
       "\n",
       "                        rmse D  \n",
       "NN 1 layers 32 units  0.174429  \n",
       "NN 2 layers 32 units  0.186808  \n",
       "NN 2 layers 64 units  0.208103  \n",
       "NN 3 layers 32 units  0.247671  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# Try different NN architectures\n",
    "architectures = [\n",
    "    (1, 32),  # 1 layer with 32 units\n",
    "    #(1, 64),  # 1 layer with 64 units \n",
    "    (2, 32),  # 2 layers with 32 units each\n",
    "    (2, 64),  # 2 layers with 64 units each\n",
    "    (3, 32),  # 3 layers with 32 units each\n",
    "    #(3, 64)   # 3 layers with 64 units each\n",
    "]\n",
    "\n",
    "results = []\n",
    "for layers, units in architectures:\n",
    "    # Create hidden layer sizes tuple based on number of layers and units\n",
    "    hidden_layer_sizes = tuple([units] * layers)\n",
    "    \n",
    "    # Create models\n",
    "    modely = make_pipeline(\n",
    "        StandardScaler(),\n",
    "        MLPRegressor(\n",
    "            hidden_layer_sizes=hidden_layer_sizes,\n",
    "            max_iter=50,\n",
    "            batch_size=32,\n",
    "            verbose=0\n",
    "        )\n",
    "    )\n",
    "    modeld = make_pipeline(\n",
    "        StandardScaler(),\n",
    "        MLPRegressor(\n",
    "            hidden_layer_sizes=hidden_layer_sizes, \n",
    "            max_iter=50,\n",
    "            batch_size=32,\n",
    "            verbose=0\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Run DML model\n",
    "    result = dml(Z, D, Y, modely, modeld, nfolds=5, classifier=False)\n",
    "    \n",
    "    # Add to summary table\n",
    "    name = f'NN {layers} layers {units} units'\n",
    "    table = pd.concat([table, summary(*result, Z, D, y, name=name)])\n",
    "\n",
    "table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowest RMSE y:  NN 3 layers 32 units\n",
      "Lowest RMSE D:  NN 1 layers 32 units\n"
     ]
    }
   ],
   "source": [
    "rmses = table.iloc[:, -2:]\n",
    "print(\"Lowest RMSE y: \", rmses.iloc[:, 0].idxmin())\n",
    "print(\"Lowest RMSE D: \", rmses.iloc[:, 1].idxmin())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the results, it seems like the NN with one layer and 32 units (number of neurons in each layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 11 {-}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DML Estimate: 0.16428903199745787\n"
     ]
    }
   ],
   "source": [
    "# Using the NN with 1 layer and 32 units\n",
    "modely = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    MLPRegressor(\n",
    "        hidden_layer_sizes=(32,),\n",
    "        max_iter=50,\n",
    "        batch_size=32,\n",
    "        verbose=0\n",
    "    )\n",
    ")\n",
    "modeld = make_pipeline(\n",
    "    StandardScaler(), \n",
    "    MLPRegressor(\n",
    "        hidden_layer_sizes=(32,),\n",
    "        max_iter=50,\n",
    "        batch_size=32,\n",
    "        verbose=0\n",
    "    )\n",
    ")\n",
    "\n",
    "# Run DML model\n",
    "result = dml(Z, D, Y, modely, modeld, nfolds=5, classifier=False)\n",
    "\n",
    "# Add to summary table\n",
    "name = 'NN 1 layer 32 units'\n",
    "table = pd.concat([table, summary(*result, Z, D, Y, name=name)])\n",
    "\n",
    "print(\"\\nDML Estimate:\", result[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 13 {-}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>birthweight</th>\n",
       "      <th>boy</th>\n",
       "      <th>married</th>\n",
       "      <th>black</th>\n",
       "      <th>age</th>\n",
       "      <th>highschool</th>\n",
       "      <th>somecollege</th>\n",
       "      <th>college</th>\n",
       "      <th>prenone</th>\n",
       "      <th>presecond</th>\n",
       "      <th>prethird</th>\n",
       "      <th>smoker</th>\n",
       "      <th>cigsdaily</th>\n",
       "      <th>weightgain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2926</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3595</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4338</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2807</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2840</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   birthweight  boy  married  black  age  highschool  somecollege  college  \\\n",
       "0         2926    0        1      0   25           1            0        0   \n",
       "1         3595    0        0      1   17           0            0        0   \n",
       "2         4338    1        1      0   21           0            0        0   \n",
       "3         2807    1        1      0   32           0            1        0   \n",
       "4         2840    1        1      0   29           1            0        0   \n",
       "\n",
       "   prenone  presecond  prethird  smoker  cigsdaily  weightgain  \n",
       "0        0          0         0       0          3          22  \n",
       "1        0          0         0       1          0          44  \n",
       "2        0          0         0       1          0          61  \n",
       "3        0          0         0       1          0          40  \n",
       "4        0          0         0       0         20          24  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing from the ps2\n",
    "\n",
    "data_qr = pd.read_csv(\"./data_qr.csv\")\n",
    "data_qr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uncontrolled Effect of Smoking on Birthweight:\n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const       3070.7236     17.864    171.894      0.000    3035.706    3105.741\n",
      "smoker       224.0707     19.128     11.715      0.000     186.577     261.565\n",
      "==============================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['boy', 'married', 'black', 'age', 'highschool', 'somecollege',\n",
       "       'college', 'prenone', 'presecond', 'prethird', 'smoker', 'cigsdaily',\n",
       "       'weightgain', 'age2', 'weightgain2'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. First, let's prepare the data by adding squared terms\n",
    "data_qr['age2'] = data_qr['age'] ** 2\n",
    "data_qr['weightgain2'] = data_qr['weightgain'] ** 2\n",
    "\n",
    "# 2. Simple uncontrolled regression of birthweight on smoking\n",
    "simple_model = sm.OLS(data_qr['birthweight'], sm.add_constant(data_qr['smoker'])).fit()\n",
    "print(\"Uncontrolled Effect of Smoking on Birthweight:\")\n",
    "print(simple_model.summary().tables[1])\n",
    "\n",
    "# 3. Prepare features for neural networks\n",
    "control_vars = data_qr.drop(columns=['birthweight']).columns\n",
    "\n",
    "control_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DML Estimate: -503.3289201204332\n"
     ]
    }
   ],
   "source": [
    "X = data_qr[control_vars].values\n",
    "y = data_qr['birthweight'].values\n",
    "d = data_qr['smoker'].values\n",
    "\n",
    "# Create models using scikit-learn's MLPRegressor\n",
    "modely = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    MLPRegressor(\n",
    "        hidden_layer_sizes=(64, 32),  # Two layers: 64 units, then 32 units\n",
    "        max_iter=50,\n",
    "        batch_size=32,\n",
    "        activation='relu',\n",
    "        verbose=0\n",
    "    )\n",
    ")\n",
    "\n",
    "modeld = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    MLPRegressor(\n",
    "        hidden_layer_sizes=(64, 32),\n",
    "        max_iter=50,\n",
    "        batch_size=32,\n",
    "        activation='relu',\n",
    "        verbose=0\n",
    "    )\n",
    ")\n",
    "\n",
    "# Run DML model\n",
    "result = dml(X, d, y, modely, modeld, nfolds=5, classifier=False)\n",
    "\n",
    "# Add to summary table\n",
    "name = 'NN 2 layers 64,32 units'\n",
    "table = pd.concat([table, summary(*result, X, d, y, name=name)])\n",
    "\n",
    "print(\"\\nDML Estimate:\", result[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Double Machine Learning (DML) estimate suggests that maternal smoking during pregnancy has a negative causal effect on infant birthweight. Specifically, we find that smoking is associated with approximately 503 grams lower birthweight, which aligns with existing medical literature. This negative relationship makes intuitive sense from a biological perspective: smoking restricts blood flow and oxygen to the developing fetus, which can impair growth and development. \n",
    "\n",
    "This is in stark contrast to our initial naive estimate which showed a positive relationship between smoking and birthweight. The difference between these estimates highlights the importance of controlling for confounding variables through methods like DML. The naive estimate was likely biased by confounders - factors that affect both smoking behavior and birthweight - whereas DML helps isolate the true causal effect by accounting for these confounding relationships. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
